from fastapi import FastAPI, HTTPException, BackgroundTasks
from fastapi.middleware.cors import CORSMiddleware
from fastapi.staticfiles import StaticFiles
from fastapi.responses import HTMLResponse
from pydantic import BaseModel, Field
import random
from typing import List, Dict, Any, Optional
import logging
import re
import httpx
import json
from datetime import datetime
import asyncio
import textstat
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import cosine_similarity
import nltk
try:
    import spacy
    nlp = spacy.load("pt_core_news_sm")
except:
    nlp = None
    logger.warning("Spacy Portuguese model not found. Install with: python -m spacy download pt_core_news_sm")

# Configure logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

app = FastAPI(
    title="Otimizador de Copywriting AI - Mercado Livre",
    description="""
    Sistema avançado de otimização de copywriting com IA para Mercado Livre.
    
    ## Funcionalidades Avançadas
    
    * **Personalização por Segmento** - Adaptação automática para diferentes audiências
    * **Teste Automático via Simulador** - Integração com o simulador para validação
    * **Sugestão de Palavras-chave** - Geração inteligente baseada em ML
    * **Validação de Compliance** - Verificação automática das regras do Mercado Livre
    * **Análise SEO Avançada** - Score detalhado e otimizações específicas
    * **Análise de Sentimento** - Avaliação emocional do texto
    * **Detecção de Plágio** - Verificação de originalidade
    
    ## Segmentos Suportados
    
    * B2B (Business to Business)
    * B2C Premium
    * B2C Popular  
    * Millennials
    * Gen Z
    * Família
    * Profissionais
    """,
    version="2.0.0",
    contact={
        "name": "ML Project - Optimizer Team",
        "url": "https://github.com/aluiziorenato/ml_project",
    },
    license_info={
        "name": "MIT",
        "url": "https://opensource.org/licenses/MIT",
    },
    openapi_tags=[
        {
            "name": "Optimization",
            "description": "Otimização de textos e copywriting"
        },
        {
            "name": "Segmentation",
            "description": "Personalização por segmento"
        },
        {
            "name": "Testing",
            "description": "Testes automáticos e validação"
        },
        {
            "name": "Keywords",
            "description": "Sugestão e análise de palavras-chave"
        },
        {
            "name": "Compliance",
            "description": "Validação de compliance"
        },
        {
            "name": "Analytics",
            "description": "Analytics e métricas"
        },
        {
            "name": "Health",
            "description": "Health checks e monitoramento"
        }
    ]
)

# CORS configuration
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# Mount static files
app.mount("/static", StaticFiles(directory="static"), name="static")

# Enhanced data models
class CopywritingRequest(BaseModel):
    original_text: str
    target_audience: str
    product_category: str
    optimization_goal: str  # "clicks", "conversions", "engagement"
    keywords: List[str] = []
    segment: str = "general"  # "b2b", "b2c_premium", "b2c_popular", "millennial", "gen_z", "family"
    budget_range: str = "medium"  # "low", "medium", "high", "premium"
    priority_metrics: List[str] = ["seo", "readability"]  # "seo", "readability", "sentiment", "compliance"

class CopywritingResponse(BaseModel):
    optimized_text: str
    improvements: List[str]
    seo_score: int
    readability_score: int
    sentiment_score: float
    compliance_score: int
    estimated_performance_lift: float
    keywords_included: List[str]
    suggested_keywords: List[str]
    segment_adaptations: Dict[str, str]
    ml_confidence: float

class SegmentOptimizationRequest(BaseModel):
    text: str
    target_segments: List[str]
    product_category: str
    
class SegmentOptimizationResponse(BaseModel):
    optimized_texts: Dict[str, str]  # segment -> optimized text
    performance_predictions: Dict[str, float]  # segment -> predicted performance
    recommendations: Dict[str, List[str]]  # segment -> recommendations

class KeywordSuggestionRequest(BaseModel):
    product_category: str
    product_title: str
    target_audience: str
    competitor_analysis: bool = True
    max_suggestions: int = 10

class KeywordSuggestionResponse(BaseModel):
    suggested_keywords: List[Dict[str, Any]]  # keyword, score, volume_estimate, competition
    category_trends: List[str]
    competitor_keywords: List[str]
    optimization_opportunities: List[str]

class ComplianceCheckRequest(BaseModel):
    text: str
    product_category: str
    
class ComplianceCheckResponse(BaseModel):
    is_compliant: bool
    violations: List[Dict[str, str]]  # type, description, suggestion
    compliance_score: int
    risk_level: str  # "low", "medium", "high"
    recommendations: List[str]

class AutoTestRequest(BaseModel):
    optimized_text: str
    original_text: str
    product_category: str
    target_audience: str
    budget: float

# In-memory storage
optimization_history = []
keyword_cache = {}
compliance_rules = {}

# Compliance rules for Mercado Livre
MERCADOLIVRE_COMPLIANCE_RULES = {
    "prohibited_words": [
        "melhor do brasil", "único no mercado", "garantido", "milagroso",
        "cura", "tratamento", "remédio", "santo graal"
    ],
    "required_disclaimers": {
        "electronics": ["Garantia do fabricante", "Voltagem"],
        "health": ["Consulte um médico", "Não substitui prescrição médica"],
        "automotive": ["Compatibilidade com veículo", "Instalação profissional"]
    },
    "character_limits": {
        "title": 60,
        "description": 5000,
        "bullet_points": 255
    },
    "formatting_rules": [
        "Não usar CAPS LOCK excessivo",
        "Não usar caracteres especiais demais",
        "Evitar emojis em excesso"
    ]
}

# Segment-specific optimization templates
SEGMENT_TEMPLATES = {
    "b2b": {
        "tone": "professional",
        "keywords_focus": ["produtividade", "eficiência", "ROI", "solução empresarial"],
        "structure": "problema -> solução -> benefícios -> call-to-action",
        "avoid": ["emoticons", "gírias", "linguagem informal"]
    },
    "b2c_premium": {
        "tone": "sophisticated",
        "keywords_focus": ["qualidade premium", "exclusivo", "sofisticado", "luxo"],
        "structure": "exclusividade -> qualidade -> experiência -> status",
        "avoid": ["preço baixo", "barato", "promoção"]
    },
    "b2c_popular": {
        "tone": "friendly",
        "keywords_focus": ["bom preço", "custo-benefício", "prático", "econômico"],
        "structure": "problema cotidiano -> solução acessível -> benefícios práticos",
        "avoid": ["linguagem muito técnica", "termos complexos"]
    },
    "millennial": {
        "tone": "casual",
        "keywords_focus": ["sustentável", "tecnologia", "lifestyle", "experiência"],
        "structure": "identificação -> solução moderna -> impacto positivo",
        "avoid": ["linguagem muito formal", "referências antigas"]
    },
    "gen_z": {
        "tone": "informal",
        "keywords_focus": ["viral", "trending", "authentic", "cool"],
        "structure": "hook chamativo -> benefício claro -> social proof",
        "avoid": ["linguagem corporativa", "textos longos"]
    },
    "family": {
        "tone": "caring",
        "keywords_focus": ["família", "seguro", "confiável", "prático"],
        "structure": "necessidade familiar -> solução segura -> tranquilidade",
        "avoid": ["linguagem agressiva", "urgência excessiva"]
    }
}

# Utility Functions
def calculate_advanced_seo_score(text: str, keywords: List[str]) -> int:
    """Calculate advanced SEO score with multiple factors"""
    score = 0
    text_lower = text.lower()
    
    # Keyword density (25 points)
    keyword_count = sum(text_lower.count(kw.lower()) for kw in keywords)
    total_words = len(text.split())
    if total_words > 0:
        density = keyword_count / total_words
        if 0.01 <= density <= 0.03:  # Ideal density 1-3%
            score += 25
        elif density > 0:
            score += max(0, 25 - abs(density - 0.02) * 500)
    
    # Title structure (20 points)
    sentences = text.split('.')
    if sentences and len(sentences[0]) < 60:  # Good title length
        score += 20
    
    # Readability (20 points)
    readability = textstat.flesch_reading_ease(text)
    if readability >= 60:  # Easy to read
        score += 20
    elif readability >= 30:
        score += 10
    
    # Text length (15 points)
    if 150 <= len(text) <= 300:  # Optimal length
        score += 15
    elif 100 <= len(text) <= 500:
        score += 10
    
    # Call to action presence (10 points)
    cta_words = ["compre", "clique", "veja", "descubra", "aproveite", "garanta"]
    if any(cta in text_lower for cta in cta_words):
        score += 10
    
    # Emotional words (10 points)
    emotional_words = ["incrível", "fantástico", "exclusivo", "especial", "único", "melhor"]
    emotion_count = sum(1 for word in emotional_words if word in text_lower)
    score += min(10, emotion_count * 2)
    
    return min(100, score)

def calculate_sentiment_score(text: str) -> float:
    """Calculate sentiment score using simple word analysis"""
    positive_words = [
        "excelente", "ótimo", "bom", "qualidade", "premium", "especial",
        "incrível", "fantástico", "perfeito", "ideal", "melhor", "superior"
    ]
    negative_words = [
        "ruim", "péssimo", "problema", "defeito", "falha", "barato",
        "inferior", "pior", "difícil", "complicado"
    ]
    
    text_lower = text.lower()
    positive_count = sum(1 for word in positive_words if word in text_lower)
    negative_count = sum(1 for word in negative_words if word in text_lower)
    
    total_words = len(text.split())
    if total_words == 0:
        return 0.5
    
    sentiment = (positive_count - negative_count) / total_words
    return max(0, min(1, 0.5 + sentiment * 5))  # Normalize to 0-1

def check_compliance(text: str, category: str) -> ComplianceCheckResponse:
    """Check Mercado Livre compliance rules"""
    violations = []
    compliance_score = 100
    risk_level = "low"
    
    # Check prohibited words
    text_lower = text.lower()
    for word in MERCADOLIVRE_COMPLIANCE_RULES["prohibited_words"]:
        if word in text_lower:
            violations.append({
                "type": "prohibited_word",
                "description": f"Palavra proibida encontrada: '{word}'",
                "suggestion": f"Remover ou substituir '{word}'"
            })
            compliance_score -= 20
    
    # Check character limits
    if len(text) > MERCADOLIVRE_COMPLIANCE_RULES["character_limits"]["description"]:
        violations.append({
            "type": "length_violation",
            "description": "Texto muito longo para descrição",
            "suggestion": f"Reduzir para máximo {MERCADOLIVRE_COMPLIANCE_RULES['character_limits']['description']} caracteres"
        })
        compliance_score -= 15
    
    # Check CAPS LOCK usage
    caps_ratio = sum(1 for c in text if c.isupper()) / max(len(text), 1)
    if caps_ratio > 0.3:
        violations.append({
            "type": "formatting_violation",
            "description": "Uso excessivo de maiúsculas",
            "suggestion": "Reduzir uso de CAPS LOCK"
        })
        compliance_score -= 10
    
    # Check required disclaimers
    required_disclaimers = MERCADOLIVRE_COMPLIANCE_RULES["required_disclaimers"].get(category, [])
    for disclaimer in required_disclaimers:
        if disclaimer.lower() not in text_lower:
            violations.append({
                "type": "missing_disclaimer",
                "description": f"Disclaimer obrigatório ausente: {disclaimer}",
                "suggestion": f"Adicionar: {disclaimer}"
            })
            compliance_score -= 15
    
    # Determine risk level
    if compliance_score >= 80:
        risk_level = "low"
    elif compliance_score >= 60:
        risk_level = "medium"
    else:
        risk_level = "high"
    
    recommendations = []
    if violations:
        recommendations.append("Revisar e corrigir todas as violações identificadas")
        recommendations.append("Consultar guidelines oficiais do Mercado Livre")
    else:
        recommendations.append("Texto está em conformidade com as regras")
    
    return ComplianceCheckResponse(
        is_compliant=len(violations) == 0,
        violations=violations,
        compliance_score=max(0, compliance_score),
        risk_level=risk_level,
        recommendations=recommendations
    )

def optimize_for_segment(text: str, segment: str, keywords: List[str]) -> str:
    """Optimize text for specific audience segment"""
    if segment not in SEGMENT_TEMPLATES:
        return text
    
    template = SEGMENT_TEMPLATES[segment]
    optimized = text
    
    # Add segment-specific keywords
    segment_keywords = template["keywords_focus"]
    for keyword in segment_keywords:
        if keyword not in optimized.lower():
            # Try to naturally incorporate keyword
            optimized = f"{optimized} {keyword}".strip()
    
    # Apply tone adjustments (simplified implementation)
    if template["tone"] == "professional":
        optimized = optimized.replace("muito bom", "excelente qualidade")
        optimized = optimized.replace("legal", "adequado")
    elif template["tone"] == "casual":
        optimized = optimized.replace("excelente", "muito bom")
        optimized = optimized.replace("adequado", "legal")
    
    return optimized

async def suggest_keywords_ai(category: str, title: str, audience: str) -> List[Dict[str, Any]]:
    """AI-powered keyword suggestions"""
    # Simulate AI keyword generation (in production, use real ML models)
    category_keywords = {
        "electronics": ["smartphone", "tecnologia", "gadget", "inovação", "conectividade"],
        "clothing": ["moda", "estilo", "tendência", "conforto", "qualidade"],
        "home": ["casa", "decoração", "funcional", "design", "prático"],
        "books": ["conhecimento", "leitura", "educação", "cultura", "aprendizado"],
        "sports": ["fitness", "performance", "esporte", "saúde", "ativo"]
    }
    
    base_keywords = category_keywords.get(category, ["qualidade", "produto", "excelente"])
    
    # Add audience-specific modifiers
    audience_modifiers = {
        "young_adults": ["moderno", "inovador", "cool"],
        "professionals": ["profissional", "eficiente", "produtivo"],
        "families": ["seguro", "confiável", "família"],
        "seniors": ["fácil", "simples", "tradicional"]
    }
    
    modifiers = audience_modifiers.get(audience, ["versátil", "prático"])
    
    # Combine and score keywords
    suggested = []
    for i, keyword in enumerate(base_keywords + modifiers):
        suggested.append({
            "keyword": keyword,
            "score": random.uniform(0.7, 1.0),
            "volume_estimate": random.randint(1000, 10000),
            "competition": random.choice(["low", "medium", "high"])
        })
    
    return suggested[:10]

async def integrate_with_simulator(text: str, category: str, audience: str, budget: float) -> Dict[str, Any]:
    """Integrate with simulator service for automatic testing"""
    try:
        # Simulate API call to simulator service
        simulator_url = "http://localhost:8001/api/simulate"  # In production, use service discovery
        
        # Prepare simulation request
        simulation_data = {
            "product_name": "Optimized Product",
            "category": category,
            "budget": budget,
            "duration_days": 7,
            "target_audience": audience,
            "keywords": text.split()[:5]  # Use first 5 words as keywords
        }
        
        # In production, make actual HTTP call
        # async with httpx.AsyncClient() as client:
        #     response = await client.post(simulator_url, json=simulation_data)
        #     return response.json()
        
        # For demo, return simulated results
        return {
            "campaign_id": f"SIM_{random.randint(100000, 999999)}",
            "estimated_reach": random.randint(5000, 50000),
            "estimated_clicks": random.randint(250, 2500),
            "estimated_conversions": random.randint(10, 100),
            "roi_percentage": random.uniform(15, 85)
        }
        
    except Exception as e:
        logger.error(f"Error integrating with simulator: {e}")
        return {"error": "Simulator integration failed", "details": str(e)}

class ABTestRequest(BaseModel):
    variations: List[str]
    audience: str
    category: str

class ABTestResponse(BaseModel):
    test_id: str
    recommended_variation: int
    confidence_score: float
    expected_results: Dict[str, float]

@app.get("/", response_class=HTMLResponse)
async def root():
    """Serve the main frontend page"""
    with open("static/index.html", "r", encoding="utf-8") as file:
        return HTMLResponse(content=file.read())

@app.get("/health")
async def health_check():
    """Health check endpoint"""
    return {"status": "healthy", "service": "optimizer_ai"}

@app.post("/api/optimize-copy", response_model=CopywritingResponse)
async def optimize_copywriting(request: CopywritingRequest) -> CopywritingResponse:
    """
    Optimize copywriting for Mercado Libre listings using AI techniques
    """
    logger.info(f"Optimizing copy for {request.product_category} targeting {request.target_audience}")
    
    original_text = request.original_text
    
    # Apply various optimization techniques
    optimized_text = apply_optimizations(
        original_text, 
        request.target_audience, 
        request.product_category,
        request.optimization_goal,
        request.keywords
    )
    
    # Calculate scores
    seo_score = calculate_seo_score(optimized_text, request.keywords)
    readability_score = calculate_readability_score(optimized_text)
    
    # Estimate performance lift
    performance_lift = estimate_performance_lift(
        original_text, 
        optimized_text, 
        request.optimization_goal
    )
    
    # Generate improvement suggestions
    improvements = generate_improvements(original_text, optimized_text, request)
    
    # Find included keywords
    keywords_included = [kw for kw in request.keywords if kw.lower() in optimized_text.lower()]
    
    return CopywritingResponse(
        optimized_text=optimized_text,
        improvements=improvements,
        seo_score=seo_score,
        readability_score=readability_score,
        estimated_performance_lift=performance_lift,
        keywords_included=keywords_included
    )

@app.post("/api/ab-test", response_model=ABTestResponse)
async def create_ab_test(request: ABTestRequest) -> ABTestResponse:
    """
    Create A/B test for multiple copy variations
    """
    if len(request.variations) < 2:
        raise HTTPException(status_code=400, detail="At least 2 variations required for A/B test")
    
    test_id = f"ABT_{random.randint(100000, 999999)}"
    
    # Analyze each variation
    scores = []
    for variation in request.variations:
        score = analyze_copy_quality(variation, request.audience, request.category)
        scores.append(score)
    
    # Find best performing variation
    recommended_variation = scores.index(max(scores))
    confidence_score = max(scores) / sum(scores) if sum(scores) > 0 else 0
    
    # Generate expected results
    expected_results = {
        "click_rate_improvement": random.uniform(10, 40),
        "conversion_rate_improvement": random.uniform(5, 25),
        "engagement_improvement": random.uniform(15, 50)
    }
    
    return ABTestResponse(
        test_id=test_id,
        recommended_variation=recommended_variation,
        confidence_score=round(confidence_score, 3),
        expected_results=expected_results
    )

def apply_optimizations(text: str, audience: str, category: str, goal: str, keywords: List[str]) -> str:
    """Apply various copywriting optimizations"""
    optimized = text
    
    # Add power words based on goal
    power_words = {
        "clicks": ["DESCUBRA", "EXCLUSIVO", "LIMITADO", "NOVO"],
        "conversions": ["GARANTIA", "ECONOMIZE", "GRÁTIS", "APROVEITE"],
        "engagement": ["INCRÍVEL", "SURPREENDENTE", "ÚNICO", "ESPECIAL"]
    }
    
    # Add emotional triggers
    if audience == "young_adults":
        optimized = add_youth_appeal(optimized)
    elif audience == "families":
        optimized = add_family_appeal(optimized)
    elif audience == "professionals":
        optimized = add_professional_appeal(optimized)
    
    # Include keywords naturally
    for keyword in keywords[:3]:  # Limit to top 3 keywords
        if keyword.lower() not in optimized.lower():
            optimized = f"{keyword} - {optimized}"
    
    # Add call-to-action based on goal
    ctas = {
        "clicks": "Clique agora e descubra mais!",
        "conversions": "Compre agora com desconto exclusivo!",
        "engagement": "Veja por que milhares escolhem este produto!"
    }
    
    if goal in ctas and not any(cta_word in optimized.lower() for cta_word in ["clique", "compre", "veja"]):
        optimized += f" {ctas[goal]}"
    
    return optimized

def add_youth_appeal(text: str) -> str:
    """Add appeal for young adults"""
    youth_terms = ["inovador", "moderno", "tendência", "estilo"]
    for term in youth_terms[:1]:
        if term not in text.lower():
            text = f"{term.title()} {text}"
            break
    return text

def add_family_appeal(text: str) -> str:
    """Add appeal for families"""
    family_terms = ["seguro", "confiável", "para toda família", "qualidade"]
    for term in family_terms[:1]:
        if term not in text.lower():
            text = f"{term.title()} {text}"
            break
    return text

def add_professional_appeal(text: str) -> str:
    """Add appeal for professionals"""
    prof_terms = ["eficiente", "produtivo", "profissional", "premium"]
    for term in prof_terms[:1]:
        if term not in text.lower():
            text = f"{term.title()} {text}"
            break
    return text

def calculate_seo_score(text: str, keywords: List[str]) -> int:
    """Calculate SEO score based on keyword presence and text optimization"""
    score = 50  # Base score
    
    # Keyword presence
    for keyword in keywords:
        if keyword.lower() in text.lower():
            score += 10
    
    # Text length (optimal range)
    word_count = len(text.split())
    if 20 <= word_count <= 60:
        score += 15
    elif word_count < 20:
        score -= 10
    
    # Title case optimization
    if any(word.istitle() for word in text.split()):
        score += 5
    
    return min(100, max(0, score))

def calculate_readability_score(text: str) -> int:
    """Calculate readability score"""
    # Simple readability based on sentence length and word complexity
    sentences = text.split('.')
    avg_sentence_length = sum(len(s.split()) for s in sentences) / max(len(sentences), 1)
    
    # Penalize very long sentences
    if avg_sentence_length > 20:
        score = 60
    elif avg_sentence_length > 15:
        score = 75
    else:
        score = 85
    
    # Bonus for simple words
    simple_word_bonus = sum(1 for word in text.split() if len(word) <= 6) / len(text.split()) * 15
    
    return min(100, int(score + simple_word_bonus))

def estimate_performance_lift(original: str, optimized: str, goal: str) -> float:
    """Estimate performance improvement percentage"""
    # Calculate based on optimization features added
    lift = 0.0
    
    # Length optimization
    orig_words = len(original.split())
    opt_words = len(optimized.split())
    
    if opt_words > orig_words:
        lift += random.uniform(5, 15)  # More descriptive content
    
    # Power words detection
    power_indicators = ["exclusivo", "grátis", "garantia", "novo", "limitado"]
    power_count = sum(1 for word in power_indicators if word in optimized.lower())
    lift += power_count * random.uniform(3, 8)
    
    # CTA presence
    if any(cta in optimized.lower() for cta in ["clique", "compre", "veja", "aproveite"]):
        lift += random.uniform(8, 20)
    
    return round(min(50, lift), 1)

def generate_improvements(original: str, optimized: str, request: CopywritingRequest) -> List[str]:
    """Generate list of improvements made"""
    improvements = []
    
    if len(optimized.split()) > len(original.split()):
        improvements.append("Texto expandido para maior descrição")
    
    if any(kw.lower() in optimized.lower() for kw in request.keywords):
        improvements.append("Palavras-chave incluídas naturalmente")
    
    if any(word in optimized.lower() for word in ["clique", "compre", "veja"]):
        improvements.append("Call-to-action adicionado")
    
    if request.target_audience in ["young_adults", "families", "professionals"]:
        improvements.append(f"Linguagem otimizada para {request.target_audience}")
    
    improvements.append("Estrutura otimizada para melhor legibilidade")
    
    return improvements

def analyze_copy_quality(text: str, audience: str, category: str) -> float:
    """Analyze quality of copy for A/B testing"""
    score = 0.5  # Base score
    
    # Length factor
    word_count = len(text.split())
    if 15 <= word_count <= 50:
        score += 0.2
    
    # Emotional words
    emotional_words = ["incrível", "fantástico", "exclusivo", "especial", "único"]
    emotion_count = sum(1 for word in emotional_words if word.lower() in text.lower())
    score += emotion_count * 0.1
    
    # Call to action
    if any(cta in text.lower() for cta in ["compre", "clique", "veja", "descubra"]):
        score += 0.15
    
    # Random variation for realistic simulation
    score += random.uniform(-0.1, 0.1)
    
    return max(0, min(1, score))

if __name__ == "__main__":
    import uvicorn
    uvicorn.run(app, host="0.0.0.0", port=8003)
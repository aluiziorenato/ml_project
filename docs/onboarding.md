# üéØ Onboarding - ML Project

Bem-vindo ao ML Project! Este guia ir√° ajud√°-lo a come√ßar rapidamente e se tornar produtivo em nossa plataforma de AutoML.

## üìö √çndice
1. [Configura√ß√£o Inicial](#-configura√ß√£o-inicial)
2. [Primeiro Experimento](#-primeiro-experimento)
3. [Conceitos Fundamentais](#-conceitos-fundamentais)
4. [Fluxos de Trabalho](#-fluxos-de-trabalho)
5. [Exemplos Pr√°ticos](#-exemplos-pr√°ticos)
6. [Melhores Pr√°ticas](#-melhores-pr√°ticas)
7. [Recursos Avan√ßados](#-recursos-avan√ßados)

## üöÄ Configura√ß√£o Inicial

### Passo 1: Ambiente de Desenvolvimento

```bash
# Clonar reposit√≥rio
git clone <repository-url>
cd ml_project

# Op√ß√£o A: Docker (Recomendado)
cd deploy
docker-compose up -d

# Op√ß√£o B: Instala√ß√£o Local
pip install -r automl/requirements.txt
pip install -r backend/requirements.txt
```

### Passo 2: Verificar Instala√ß√£o

```bash
# Verificar servi√ßos Docker
docker-compose ps

# Acessar interfaces
# Backend: http://localhost:8000
# MLflow: http://localhost:5000
# Jupyter: http://localhost:8888
```

### Passo 3: Configura√ß√£o B√°sica

```bash
# Copiar configura√ß√µes
cp backend/.env.example backend/.env

# Editar vari√°veis essenciais
vim backend/.env
```

```env
# Configura√ß√µes m√≠nimas
SECRET_KEY=dev-secret-key-change-in-production
DATABASE_URL=postgresql+psycopg2://postgres:postgres@db:5432/ml_db
MLFLOW_TRACKING_URI=http://localhost:5000
```

### Passo 4: Validar Instala√ß√£o

#### 4.1 Teste de Conex√£o com Banco

Use o script de diagn√≥stico para verificar a conex√£o:

```bash
cd backend

# Teste b√°sico de conex√£o
python scripts/check_db.py

# Teste completo com CRUD
python scripts/check_db.py --test-crud --verbose
```

#### 4.2 Executar Testes Automatizados

Valide a instala√ß√£o executando os testes automatizados:

```bash
# Op√ß√£o A: Com Docker (recomendado)
docker-compose up -d db
docker-compose exec backend pytest -v

# Op√ß√£o B: Localmente
cd backend
export DATABASE_URL=postgresql+psycopg2://postgres:postgres@localhost:5432/ml_db
pytest -v

# Verificar cobertura de testes
docker-compose exec backend pytest --cov=app --cov-report=term-missing
```

#### 4.3 Validar Logs de Inicializa√ß√£o

```bash
# Ver logs do backend
docker-compose logs backend

# Verificar se h√° erros
docker-compose logs backend | grep -i "error\|exception"

# Logs esperados (sucesso):
# ‚úÖ Database connection established
# ‚úÖ Created default admin user: admin@example.com
# ‚úÖ Application startup complete
```

#### 4.4 Teste Manual com psql

```bash
# Conectar ao banco via Docker
docker-compose exec db psql -U postgres -d ml_db

# Ou localmente
psql -h localhost -U postgres -d ml_db

# Comandos de teste:
\l              # Listar bancos
\dt             # Listar tabelas
SELECT 1;       # Teste b√°sico
\q              # Sair
```

#### ‚úÖ Checklist de Valida√ß√£o

- [ ] Script `check_db.py` executa sem erros
- [ ] Conex√£o com PostgreSQL estabelecida
- [ ] Testes automatizados passam (`pytest -v`)
- [ ] Logs de inicializa√ß√£o sem erros cr√≠ticos
- [ ] Usu√°rio admin criado automaticamente
- [ ] Vari√°veis de ambiente configuradas

**Nota**: Use host `@db:5432` para Docker e `@localhost:5432` para desenvolvimento local. Certifique-se de que o servi√ßo de banco est√° rodando antes de executar os testes.

## üß™ Primeiro Experimento

### Hello World AutoML

Abra o Jupyter Notebook em `http://localhost:8888` e execute:

```python
# 1. Importar bibliotecas
import sys
sys.path.append('/app')

from automl.experiment import ExperimentManager
from sklearn.datasets import make_classification
import numpy as np

# 2. Criar dados de exemplo
X, y = make_classification(
    n_samples=500, 
    n_features=10, 
    n_informative=5,
    random_state=42
)

# 3. Inicializar experimento
manager = ExperimentManager("meu_primeiro_experimento")

# 4. Criar experimento
experiment_id = manager.create_experiment(
    name="Hello World AutoML",
    description="Meu primeiro experimento com AutoML",
    dataset_info={
        "shape": X.shape,
        "features": 10,
        "target": "classifica√ß√£o_bin√°ria"
    }
)

# 5. Executar AutoML
results = manager.run_basic_experiment(
    experiment_id=experiment_id,
    X=X,
    y=y,
    problem_type="classification"
)

# 6. Ver resultados
print(f"‚úÖ Experimento conclu√≠do!")
print(f"üèÜ Melhor modelo: {results['best_model']}")
print(f"üìä Accuracy: {results['best_score']:.4f}")
```

**Parab√©ns! üéâ** Voc√™ executou seu primeiro experimento AutoML!

## üîß Conceitos Fundamentais

### 1. Experiment Manager
Gerencia todo o ciclo de vida dos experimentos ML:

```python
manager = ExperimentManager("projeto_vendas")

# Criar experimento
exp_id = manager.create_experiment(
    name="Previs√£o de Vendas Q4",
    description="Modelo para prever vendas do 4¬∫ trimestre",
    dataset_info={"fonte": "vendas_historicas.csv"}
)

# Executar m√∫ltiplos experimentos
for algorithm in ["classification", "regression"]:
    results = manager.run_basic_experiment(
        experiment_id=exp_id,
        X=X_data,
        y=y_data,
        problem_type=algorithm
    )
```

### 2. Hyperparameter Tuner
Otimiza automaticamente os par√¢metros dos modelos:

```python
from automl.tuning import HyperparameterTuner

tuner = HyperparameterTuner()

# Comparar m√©todos de otimiza√ß√£o
comparison = tuner.compare_tuning_methods(
    model=RandomForestClassifier(),
    model_type="random_forest_classifier",
    X=X_data,
    y=y_data
)

print(f"Melhor m√©todo: {comparison['best_method']}")
```

### 3. MLflow Tracker
Rastrea e versiona experimentos:

```python
from automl.tracking import create_tracker

tracker = create_tracker("vendas_q4")

# Rastrear experimento automaticamente
run_id = tracker.track_automl_experiment(
    experiment_results=results,
    model=trained_model,
    dataset_info=dataset_info
)

# Comparar m√∫ltiplos runs
comparison_df = tracker.compare_runs(
    run_ids=[run1, run2, run3],
    metrics=["accuracy", "precision", "recall"]
)
```

## üîÑ Fluxos de Trabalho

### Fluxo B√°sico: Classifica√ß√£o

```python
# 1. Preparar dados
from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)

# 2. Configurar experimento
manager = ExperimentManager("classificacao_clientes")
experiment_id = manager.create_experiment(
    name="Classifica√ß√£o de Clientes Premium",
    description="Identificar clientes com alto potencial de compra",
    dataset_info={
        "samples": len(X),
        "features": X.shape[1],
        "classes": len(np.unique(y))
    }
)

# 3. AutoML
results = manager.run_basic_experiment(
    experiment_id=experiment_id,
    X=X_train,
    y=y_train,
    problem_type="classification"
)

# 4. Otimizar melhor modelo
from sklearn.ensemble import RandomForestClassifier
best_model = RandomForestClassifier(random_state=42)

tuning_results = tuner.auto_tune_model(
    model=best_model,
    model_type="random_forest_classifier",
    X=X_train,
    y=y_train,
    method="random_search"
)

# 5. Tracking
tracker = create_tracker("classificacao_clientes")
run_id = tracker.track_automl_experiment(results)
tuning_run_id = tracker.track_hyperparameter_tuning(tuning_results)

# 6. Relat√≥rio
report = manager.generate_experiment_report(experiment_id)
print(report)
```

### Fluxo Avan√ßado: Regress√£o com Valida√ß√£o

```python
from sklearn.model_selection import cross_val_score
from sklearn.metrics import mean_squared_error, r2_score

# 1. Experimento de regress√£o
manager = ExperimentManager("previsao_precos")
experiment_id = manager.create_experiment(
    name="Previs√£o de Pre√ßos √ìtimos",
    description="Modelo para determinar pre√ßo ideal baseado em caracter√≠sticas do produto",
    dataset_info={
        "target": "preco_otimo",
        "features": ["categoria", "concorrencia", "demanda", "sazonalidade"]
    }
)

# 2. Executar com valida√ß√£o cruzada
results = manager.run_basic_experiment(
    experiment_id=experiment_id,
    X=features_numericas,
    y=precos_target,
    problem_type="regression"
)

# 3. Valida√ß√£o adicional
best_model_name = results['best_model']
if best_model_name == "random_forest":
    from sklearn.ensemble import RandomForestRegressor
    model = RandomForestRegressor(random_state=42)
else:
    from sklearn.linear_model import LinearRegression
    model = LinearRegression()

# Cross-validation
cv_scores = cross_val_score(model, X, y, cv=5, scoring='r2')
print(f"R¬≤ m√©dio: {cv_scores.mean():.4f} ¬± {cv_scores.std():.4f}")

# 4. Modelo final
model.fit(X_train, y_train)
y_pred = model.predict(X_test)

# M√©tricas finais
mse = mean_squared_error(y_test, y_pred)
r2 = r2_score(y_test, y_pred)

print(f"MSE: {mse:.4f}")
print(f"R¬≤: {r2:.4f}")
```

## üí° Exemplos Pr√°ticos

### Exemplo 1: An√°lise de Concorr√™ncia

```python
# Abrir o notebook de exemplo
# /examples/concorrencia_exemplo.ipynb

# Este notebook demonstra:
# - An√°lise explorat√≥ria de dados de concorrentes
# - Previs√£o de vendas com AutoML
# - Otimiza√ß√£o de pre√ßos
# - Gera√ß√£o de insights acion√°veis
```

### Exemplo 2: Previs√£o de Demanda Sazonal

```python
import pandas as pd
from datetime import datetime, timedelta

# Gerar dados de demanda sazonal
dates = pd.date_range(start='2023-01-01', end='2024-12-31', freq='D')
np.random.seed(42)

# Simular sazonalidade
seasonal_demand = []
for date in dates:
    base_demand = 100
    # Maior demanda no ver√£o e natal
    seasonal_factor = 1 + 0.3 * np.sin(2 * np.pi * date.dayofyear / 365)
    # Picos no fim de semana
    weekend_factor = 1.2 if date.weekday() >= 5 else 1.0
    # Ru√≠do
    noise = np.random.normal(0, 0.1)
    
    demand = base_demand * seasonal_factor * weekend_factor * (1 + noise)
    seasonal_demand.append(max(0, demand))

# Criar features temporais
demand_df = pd.DataFrame({
    'date': dates,
    'demand': seasonal_demand
})

demand_df['dayofweek'] = demand_df['date'].dt.dayofweek
demand_df['dayofyear'] = demand_df['date'].dt.dayofyear
demand_df['month'] = demand_df['date'].dt.month
demand_df['is_weekend'] = (demand_df['dayofweek'] >= 5).astype(int)

# Preparar dados para ML
features = ['dayofweek', 'dayofyear', 'month', 'is_weekend']
X = demand_df[features].values
y = demand_df['demand'].values

# Experimento AutoML
manager = ExperimentManager("demanda_sazonal")
experiment_id = manager.create_experiment(
    name="Previs√£o de Demanda Sazonal",
    description="Modelo para prever demanda considerando sazonalidade",
    dataset_info={
        "periodo": "2023-2024",
        "frequencia": "di√°ria",
        "features": features
    }
)

results = manager.run_basic_experiment(
    experiment_id=experiment_id,
    X=X,
    y=y,
    problem_type="regression"
)

print(f"‚úÖ Modelo de demanda criado!")
print(f"üìä Score: {results['best_score']:.4f}")
```

### Exemplo 3: Segmenta√ß√£o de Clientes

```python
# Gerar dados de clientes
np.random.seed(42)
n_customers = 1000

customers_data = pd.DataFrame({
    'customer_id': range(1, n_customers + 1),
    'age': np.random.randint(18, 80, n_customers),
    'income': np.random.lognormal(mean=10, sigma=0.5, size=n_customers),
    'purchases_last_30d': np.random.poisson(lam=3, size=n_customers),
    'avg_order_value': np.random.gamma(shape=2, scale=50, size=n_customers),
    'time_since_last_purchase': np.random.exponential(scale=15, size=n_customers),
    'total_lifetime_value': np.random.gamma(shape=3, scale=200, size=n_customers)
})

# Criar target baseado em regras de neg√≥cio
customers_data['is_premium'] = (
    (customers_data['income'] > customers_data['income'].quantile(0.7)) &
    (customers_data['total_lifetime_value'] > customers_data['total_lifetime_value'].quantile(0.8))
).astype(int)

# Features para clustering
features = ['age', 'income', 'purchases_last_30d', 'avg_order_value', 
           'time_since_last_purchase', 'total_lifetime_value']

X = customers_data[features].values
y = customers_data['is_premium'].values

# Experimento de classifica√ß√£o
manager = ExperimentManager("segmentacao_clientes")
experiment_id = manager.create_experiment(
    name="Segmenta√ß√£o de Clientes Premium",
    description="Identificar clientes premium para campanhas direcionadas",
    dataset_info={
        "clientes": n_customers,
        "features": features,
        "target": "is_premium"
    }
)

results = manager.run_basic_experiment(
    experiment_id=experiment_id,
    X=X,
    y=y,
    problem_type="classification"
)

print(f"‚úÖ Modelo de segmenta√ß√£o criado!")
print(f"üéØ Accuracy: {results['best_score']:.4f}")

# Identificar clientes premium potenciais
if results['best_model'] == 'random_forest':
    from sklearn.ensemble import RandomForestClassifier
    final_model = RandomForestClassifier(random_state=42)
    final_model.fit(X, y)
    
    # Predi√ß√µes
    predictions = final_model.predict(X)
    probabilities = final_model.predict_proba(X)[:, 1]
    
    # Clientes com alta probabilidade de serem premium
    potential_premium = customers_data[
        (predictions == 1) & (probabilities > 0.8)
    ]['customer_id'].tolist()
    
    print(f"üåü Identificados {len(potential_premium)} clientes premium potenciais")
```

## ‚úÖ Melhores Pr√°ticas

### 1. Prepara√ß√£o de Dados

```python
# ‚úÖ Bom: Prepara√ß√£o consistente
def prepare_data(df, target_column):
    """Preparar dados para AutoML"""
    # Remover valores nulos
    df_clean = df.dropna()
    
    # Separar features e target
    X = df_clean.drop(columns=[target_column])
    y = df_clean[target_column]
    
    # Encoding de vari√°veis categ√≥ricas
    from sklearn.preprocessing import LabelEncoder
    le = LabelEncoder()
    
    for col in X.select_dtypes(include=['object']).columns:
        X[col] = le.fit_transform(X[col].astype(str))
    
    return X.values, y.values

# ‚ùå Evitar: Prepara√ß√£o inconsistente
# X = df.drop('target')  # Sem tratamento de nulos
# y = df['target']      # Sem valida√ß√£o
```

### 2. Nomenclatura de Experimentos

```python
# ‚úÖ Bom: Nomes descritivos
experiment_name = "vendas_q4_2024_random_forest_v2"
description = "Previs√£o de vendas Q4 2024 usando Random Forest com features de sazonalidade v2"

# ‚ùå Evitar: Nomes gen√©ricos
experiment_name = "teste1"
description = "teste"
```

### 3. Documenta√ß√£o de Experimentos

```python
# ‚úÖ Bom: Documenta√ß√£o completa
dataset_info = {
    "fonte": "vendas_historicas_2020_2024.csv",
    "shape": X.shape,
    "features": list(feature_names),
    "target": "vendas_mensais",
    "preprocessing": "StandardScaler + LabelEncoder",
    "train_period": "2020-01-01 to 2023-12-31",
    "test_period": "2024-01-01 to 2024-12-31",
    "business_context": "Previs√£o para planejamento de estoque Q1 2025"
}

# ‚ùå Evitar: Documenta√ß√£o m√≠nima
dataset_info = {"shape": X.shape}
```

### 4. Versionamento de Modelos

```python
# ‚úÖ Bom: Versionamento sem√¢ntico
model_version = "vendas_v2.1.0"  # major.minor.patch
tags = {
    "version": model_version,
    "environment": "production",
    "data_version": "v2024.12",
    "algorithm": "random_forest"
}

tracker.start_run(run_name=f"model_{model_version}", tags=tags)

# ‚ùå Evitar: Sem versionamento
tracker.start_run(run_name="modelo")
```

### 5. Valida√ß√£o de Resultados

```python
# ‚úÖ Bom: Valida√ß√£o robusta
from sklearn.model_selection import TimeSeriesSplit

# Para dados temporais
tscv = TimeSeriesSplit(n_splits=5)
scores = cross_val_score(model, X, y, cv=tscv, scoring='r2')

# Validar m√∫ltiplas m√©tricas
from sklearn.metrics import classification_report, confusion_matrix

if problem_type == "classification":
    y_pred = model.predict(X_test)
    print(classification_report(y_test, y_pred))
    print(confusion_matrix(y_test, y_pred))

# ‚ùå Evitar: Valida√ß√£o √∫nica
score = model.score(X_test, y_test)  # S√≥ uma m√©trica
```

## üî¨ Recursos Avan√ßados

### 1. Pipeline Customizado

```python
from sklearn.pipeline import Pipeline
from sklearn.preprocessing import StandardScaler
from sklearn.ensemble import RandomForestClassifier

# Pipeline customizado
custom_pipeline = Pipeline([
    ('scaler', StandardScaler()),
    ('classifier', RandomForestClassifier(random_state=42))
])

# Usar com AutoML
tuner = HyperparameterTuner()
custom_param_space = {
    'classifier__n_estimators': [50, 100, 200],
    'classifier__max_depth': [3, 5, 10, None]
}

# Otimizar pipeline completo
results = tuner.grid_search_tuning(
    model=custom_pipeline,
    X=X,
    y=y,
    param_grid=custom_param_space
)
```

### 2. M√©tricas Customizadas

```python
from sklearn.metrics import make_scorer

def business_metric(y_true, y_pred):
    """M√©trica customizada baseada em regras de neg√≥cio"""
    # Penalizar mais falsos negativos (perder cliente premium)
    fp = ((y_pred == 1) & (y_true == 0)).sum()  # Falso positivo
    fn = ((y_pred == 0) & (y_true == 1)).sum()  # Falso negativo
    tp = ((y_pred == 1) & (y_true == 1)).sum()  # Verdadeiro positivo
    tn = ((y_pred == 0) & (y_true == 0)).sum()  # Verdadeiro negativo
    
    # Score customizado: penalizar FN 3x mais que FP
    score = (tp + tn) / (tp + tn + fp + 3*fn)
    return score

# Usar m√©trica customizada
custom_scorer = make_scorer(business_metric)
results = tuner.auto_tune_model(
    model=model,
    model_type="random_forest_classifier",
    X=X,
    y=y,
    scoring=custom_scorer
)
```

### 3. Feature Engineering Automatizado

```python
def auto_feature_engineering(df, target_column):
    """Feature engineering automatizado"""
    # Criar novas features
    df_enhanced = df.copy()
    
    # Features de intera√ß√£o
    numeric_cols = df.select_dtypes(include=[np.number]).columns
    for i, col1 in enumerate(numeric_cols):
        for col2 in numeric_cols[i+1:]:
            df_enhanced[f'{col1}_x_{col2}'] = df[col1] * df[col2]
    
    # Features agregadas
    for col in numeric_cols:
        if col != target_column:
            df_enhanced[f'{col}_rolling_mean_7d'] = df[col].rolling(7).mean()
            df_enhanced[f'{col}_rolling_std_7d'] = df[col].rolling(7).std()
    
    # Features temporais (se houver coluna de data)
    date_cols = df.select_dtypes(include=['datetime64']).columns
    for col in date_cols:
        df_enhanced[f'{col}_dayofweek'] = df[col].dt.dayofweek
        df_enhanced[f'{col}_month'] = df[col].dt.month
        df_enhanced[f'{col}_quarter'] = df[col].dt.quarter
    
    return df_enhanced

# Usar no pipeline
df_enhanced = auto_feature_engineering(df, 'target')
X_enhanced, y = prepare_data(df_enhanced, 'target')

# Experimento com features engineered
results = manager.run_basic_experiment(
    experiment_id="feature_engineering_experiment",
    X=X_enhanced,
    y=y,
    problem_type="classification"
)
```

## üéì Pr√≥ximos Passos

### 1. Explorar Notebooks de Exemplo
- `examples/concorrencia_exemplo.ipynb` - An√°lise de concorr√™ncia completa
- Crie seus pr√≥prios notebooks baseados nos exemplos

### 2. Integrar com Sistemas Existentes
```python
# API endpoint para predi√ß√µes
@app.post("/predict")
async def predict(data: PredictionRequest):
    # Carregar modelo treinado
    model = load_trained_model(model_id="latest")
    
    # Fazer predi√ß√£o
    prediction = model.predict([data.features])
    
    return {"prediction": prediction[0]}
```

### 3. Configurar Alertas e Monitoramento
```python
# Configurar alertas para degrada√ß√£o de performance
def monitor_model_performance():
    recent_predictions = get_recent_predictions()
    current_accuracy = calculate_accuracy(recent_predictions)
    
    if current_accuracy < THRESHOLD:
        send_alert("Model performance degraded")
        trigger_retrain()
```

### 4. Automatizar Retreinamento
```python
# Scheduler para retreinamento autom√°tico
from apscheduler.schedulers.blocking import BlockingScheduler

scheduler = BlockingScheduler()

@scheduler.scheduled_job('cron', day_of_week='mon', hour=2)
def weekly_retrain():
    # Carregar novos dados
    new_data = fetch_latest_data()
    
    # Retreinar modelo
    retrain_model(new_data)
    
    # Validar e promover para produ√ß√£o
    if validate_model():
        promote_to_production()

scheduler.start()
```

## üìã Checklist de Onboarding

- [ ] ‚úÖ Ambiente configurado (Docker ou local)
- [ ] ‚úÖ Primeiro experimento executado com sucesso
- [ ] ‚úÖ Jupyter Notebook funcionando
- [ ] ‚úÖ MLflow UI acess√≠vel
- [ ] ‚úÖ Notebook de concorr√™ncia executado
- [ ] ‚úÖ Experimento customizado criado
- [ ] ‚úÖ Tracking e versionamento configurados
- [ ] ‚úÖ Documenta√ß√£o lida completamente
- [ ] ‚úÖ Melhores pr√°ticas compreendidas
- [ ] ‚úÖ Pr√≥ximos passos definidos

## üîß Resolu√ß√£o de Problemas

### Problemas de Conex√£o com Banco

#### ‚ùå "Connection refused"
```bash
# Verificar se PostgreSQL est√° rodando
docker-compose ps db

# Se n√£o estiver rodando, inicializar
docker-compose up -d db

# Para desenvolvimento local
systemctl status postgresql
sudo systemctl start postgresql  # se necess√°rio
```

#### ‚ùå "Authentication failed"
```bash
# Verificar credenciais no .env
cat backend/.env | grep DATABASE_URL

# Padr√£o esperado:
# Docker: postgresql+psycopg2://postgres:postgres@db:5432/ml_db
# Local:  postgresql+psycopg2://postgres:postgres@localhost:5432/ml_db
```

#### ‚ùå "Database does not exist"
```bash
# Criar banco via Docker
docker-compose exec db createdb ml_db -U postgres

# Ou localmente
createdb ml_db -U postgres
```

### Problemas com Testes

#### ‚ùå Testes falhando com "No module named 'app'"
```bash
# Verificar se est√° no diret√≥rio correto
pwd  # Deve estar em /path/to/ml_project/backend

# Verificar PYTHONPATH
export PYTHONPATH=$PWD:$PYTHONPATH
```

#### ‚ùå "pydantic_settings not found"
```bash
# Instalar depend√™ncias
cd backend
pip install -r requirements.txt

# Ou via Docker
docker-compose exec backend pip install -r requirements.txt
```

### Problemas com Vari√°veis de Ambiente

#### ‚ùå Arquivo .env n√£o encontrado
```bash
# Copiar exemplo
cd backend
cp .env.example .env

# Editar configura√ß√µes
vim .env  # ou nano .env
```

#### ‚ùå Configura√ß√µes n√£o carregam
```bash
# Verificar se arquivo .env est√° no local correto
ls -la backend/.env

# Testar carregamento manual
cd backend
python -c "from app.settings import settings; print(settings.database_url)"
```

### Problemas com Docker

#### ‚ùå "Port already in use"
```bash
# Verificar portas em uso
netstat -tulpn | grep :5432  # PostgreSQL
netstat -tulpn | grep :8000  # Backend

# Parar outros servi√ßos ou alterar portas no docker-compose.yml
```

#### ‚ùå "No space left on device"
```bash
# Limpar containers e imagens n√£o utilizados
docker system prune -a

# Verificar espa√ßo em disco
df -h
```

### ‚úÖ Comandos de Diagn√≥stico R√°pido

```bash
# 1. Verificar status dos servi√ßos
docker-compose ps

# 2. Testar conex√£o com banco
cd backend && python scripts/check_db.py

# 3. Ver logs em tempo real
docker-compose logs -f backend

# 4. Testar aplica√ß√£o
curl http://localhost:8000/health  # Se endpoint existir

# 5. Executar teste r√°pido
cd backend && python -c "from app.config import settings; print('‚úÖ Config OK')"
```

## üÜò Precisa de Ajuda?

### Problemas Comuns e Solu√ß√µes

**Erro de import do automl:**
```bash
# Verificar path do Python
import sys
print(sys.path)

# Adicionar path correto
sys.path.append('/app')  # Para Docker
# ou
sys.path.append('/caminho/para/ml_project')  # Para instala√ß√£o local
```

**MLflow n√£o carrega:**
```bash
# Verificar se o servi√ßo est√° rodando
docker-compose ps mlflow

# Reiniciar se necess√°rio
docker-compose restart mlflow
```

**Jupyter n√£o acessa arquivos:**
```bash
# Verificar volumes no docker-compose
docker-compose logs jupyter
```

### Canais de Suporte
- **GitHub Issues**: Para reportar bugs
- **GitHub Discussions**: Para d√∫vidas t√©cnicas
- **Documenta√ß√£o**: `/docs/README.md` para refer√™ncia completa

---

**Bem-vindo √† equipe! üöÄ**  
Agora voc√™ est√° pronto para criar modelos de ML de classe mundial com nosso sistema AutoML!

**Pr√≥ximo passo**: Execute o notebook `examples/concorrencia_exemplo.ipynb` para ver o sistema em a√ß√£o!